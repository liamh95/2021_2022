\documentclass[12pt]{article}
\usepackage[left=0.75in,right=0.75in,top=0.75in,bottom=0.75in,
            footskip=0.25in]{geometry}
\usepackage{graphicx,float,hyperref} 
\usepackage{amsmath,amsthm,amssymb,amsfonts,geometry,mathtools,enumerate,bbm}
\usepackage{algpseudocode}
\usepackage{fancyvrb}
 
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}
\newtheorem{property}[theorem]{Property}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{example}[theorem]{Example}
\newtheorem{examples}[theorem]{Examples}


\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{exercises}[theorem]{Exercises}

\newcommand{\Bin}{\ensuremath{\textrm{Bin}}}
 
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
 
% \newenvironment{problem}[2][Problem]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever
 
\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{Math 173A}
\author{Liam Hardiman}

\maketitle

\begin{abstract}
    I'm writing these lecture notes for UC Irvine's Math 173A course, taught in the summer of 2022.
    This is a five-ish week course where I plan to get through the first three chapters of Hoffstein, Pipher and Silverman's book \cite{HPS}.
    The class structure consists of a two hour lecture followed by a one hour discussion section three days a week.
    I'm aiming to get through two sections of the book per lecture with a midterm after chapter 2.

\end{abstract}


\tableofcontents


\section{An Introduction to Cryptography}
\subsection{Simple Substitution Ciphers}
One of history's oldest examples of encrypting messages is the \emph{shift cipher}, sometimes called the \emph{Caesar cipher} after Julius Caesar, who allegedly used it to encrypt the orders he'd send to his troops.
To encrypt a message, simply shift each letter of the plaintext forward in the alphabet by three, wrapping around if the shifted letter goes past \texttt{Z}. For example, if the key\footnote{We won't rigorously define what ``plaintext'', ``ciphertext'' or ``key'' mean. You can think of the plaintext as being being the human-readable or usable message (maybe consisting of letters or a number) and the ciphertext as being some unreadable sequence of letters or numbers. Then you can think of the key as being some piece of information that tells you how to convert between plain- and ciphertext.} is \texttt{3} and our plaintext is \texttt{hello, world}, then we have the following ciphertext.

\begin{center}
    \texttt{hello world} $\mapsto$ \texttt{KHOOR ZRUOG}
\end{center}

Conversely, if we know the key is \texttt{3} and we're given the ciphertext \texttt{ZHGQH VGDB}, then we simply shift backwards by 3 to obtain the plaintext.

\begin{center}
    \texttt{ZHGQH VGDB} $\mapsto$ \texttt{wedne sday}
\end{center}

One advantage to the shift cipher is that it's really easy to encrypt and decrypt messages if the key is known.
The main disadvantage is that it's only slightly challenging (more annoying than challenging) for an adversary to decrypt messages even if they don't know the key.
If we use the English alphabet, then there are only 26 possible keys and it doesn't take too long to try them all (a few minutes by hand, a fraction of a second even with bad code).
This trial and error method of trying all possible keys, sometimes called \emph{brute forcing}, works because it's pretty unlikely that decrypting with two different keys will yield two plaintexts that are both readable.
For example, suppose we happen upon the following ciphertext
\begin{center}
    \texttt{XPPEE ZXZCC ZH}.
\end{center}
If we suspect that this ciphertext came from a shift cipher, we can just try all possible un-shifts to get the following possible plaintexts.

\begin{center}
    \begin{tabular}{|c|c||c|c|}
    \hline
    key & plaintext & key & plaintext\\
    \texttt{1} & \texttt{woodd ywybb yg} & \texttt{14} & \texttt{jbbqq ljloo lt}\\
    \texttt{2} & \texttt{vnncc xvxaa xf} & \texttt{15} & \texttt{iaapp kiknn ks}\\
    \texttt{3} & \texttt{ummbb wuwzz we} & \texttt{16} & \texttt{hzzoo jhjmm jr}\\
    \texttt{4} & \texttt{tllaa vtvyy vd} & \texttt{17} & \texttt{gyynn igill iq}\\
    \texttt{5} & \texttt{skkzz usuxx uc} & \texttt{18} & \texttt{fxxmm hfhkk hp}\\
    \texttt{6} & \texttt{rjjyy trtww tb} & \texttt{19} & \texttt{ewwll gegjj go}\\
    \texttt{7} & \texttt{qiixx sqsvv sa} & \texttt{20} & \texttt{dvvkk fdfii fn}\\
    \texttt{8} & \texttt{phhww rpruu rz} & \texttt{21} & \texttt{cuujj ecehh em}\\
    \texttt{9} & \texttt{oggvv qoqtt qy} & \texttt{22} & \texttt{bttii dbdgg dl}\\
    \texttt{10} & \texttt{nffuu pnpss px} & \texttt{23} & \texttt{asshh cacff ck}\\
    \texttt{11} & \texttt{meett omorr ow} & \texttt{24} & \texttt{zrrgg bzbee bj}\\
    \texttt{12} & \texttt{lddss nlnqq nv} & \texttt{25} & \texttt{yqqff ayadd ai}\\
    \texttt{13} & \texttt{kccrr mkmpp mu} & \texttt{ } & \texttt{ }\\
    \hline
    \end{tabular}
\end{center}

The only plaintext here that's even remotely readable is \texttt{meett omorr ow}, corresponding to a key of \texttt{11}.
This process of decrypting a ciphertext without knowing the key in advance is called \emph{cryptanalysis}.

Notice that with a shift cipher, each instance of \texttt{a} encrypts to the same character, and so on.
In this setting, once we know what one character maps to, then we know what all the other characters map to as well.
E.g. if we know that \texttt{m} maps to \texttt{X}, then we know that the cipher shifts each character forward by 11, which immediately tells us that \texttt{a} maps to \texttt{L}, and so on.
A more general \emph{simple substitution cipher} decouples the encryptions of different letters, e.g. each \texttt{a} maps to \texttt{C} and each \texttt{b} maps to \texttt{J}, etc.

\begin{question}
    Explain why this particular substitution cipher is not a shift cipher.
\end{question}

\begin{question}
    How many possible keys are there in a substitution cipher? Hint: think of encryption as a function. What properties should this function have?
\end{question}

What would cryptanalysis of a simple substitution cipher look like?
There are more than $10^{26}$ keys in this case.
If we could try a million keys every second, it would still take more than $10^{13}$ years to try them all, so the brute-force solution is infeasible.
Despite the huge number of possible keys, simple substitution ciphers are often really easy to cryptanalyze in practice with simple \emph{frequency analysis}.
The idea is that if the plaintext is more than a few sentences long, then one might expect to see a lot of \texttt{e}'s, \texttt{t}'s and \texttt{a}'s and not many \texttt{z}'s or \texttt{q}'s.
Consequently, if we look at the frequencies of the letters in the ciphertext, it would be reasonable to guess that the most common ciphertext letters correspond to the most common plaintext letters.

For example, suppose we intercept the following message.

\begin{center}
\begin{BVerbatim}
    LWNSOZ BNWVWB AYBNVB SQWVUO HWDIZW RBBNPB POOUWR PAWXAW
    PBWZWM YPOBNP BBNWJP AWWRZS LWZQJB NVIAXA WPBSAL IBNXWA
    BPIRYR POIWRP QOWAIE NBVBNP BPUSRE BNWVWP AWOIHW OIQWAB
    JPRZBN WFYAVY IBSHNP FFIRWV VBNPBB SVWXYA WBNWVW AIENBV
    ESDWAR UWRBVP AWIRVB IBYBWZ PUSREU WRZWAI DIREBH WIATYV
    BFSLWA VHASUB NWXSRV WRBSHB NWESDW ARWZBN PBLNWR WDWAPR
    JHSAUS HESDWA RUWRBQ WXSUWV ZWVBAY XBIDWS HBNWVW WRZVIB
    IVBNVA IENBSH BNWFWS FOWBSP OBWASA BSPQSO IVNIBP RZBSIR
    VBIBYB WRWLES DWARUW RBOPJI REIBVH SYRZPB ISRSRV YXNFAI
    RXIFOW VPRZSA EPRIKI REIBVF SLWAVI RVYXNH SAUPVB SVWWUU
    SVBOIC WOJBSW HHWXBB NWIAVP HWBJPR ZNPFFI RWVV
\end{BVerbatim}
\end{center}

Let's arrange the letters in the ciphertext by frequency.

\begin{center}
\begin{tabular}{c c c c c c c c c c c}
    \texttt{W} & \texttt{B} & \texttt{R} & \texttt{S} & \texttt{I} & \texttt{V} & \texttt{A} & \texttt{P} & \texttt{N} & \texttt{O} & $\cdots$\\
    76 & 64 & 39 & 36 & 36 & 35 & 34 & 32 & 30 & 16 & $\cdots$
\end{tabular}

\end{center}

The letters in standard English text have the following frequencies.

\begin{center}
\begin{tabular}{c c c c c c c c c c c}
    \texttt{E} & \texttt{T} & \texttt{A} & \texttt{O} & \texttt{N} & \texttt{R} & \texttt{I} & \texttt{S} & \texttt{H} & \texttt{D} & $\cdots$\\
    .131 & .105 & .082 & .080 & .071 & .068 & .064 & .061 & .053 & .038 & $\cdots$
\end{tabular}
\end{center}

Since the letter \texttt{W} appears much more frequently than the other letters in the ciphertext, it tips us off that we might be dealing with a substitution cipher and that an \texttt{e} in the plaintext probably maps to a \texttt{W} in the ciphertext. It's also reasonable to guess that the letters \texttt{B}, \texttt{R}, \texttt{S} and \texttt{I} correspond to the letters \texttt{t}, \texttt{a}, \texttt{o} and \texttt{i} in some order.

Looking at individual letter frequencies lets us get our foot in the door, but it doesn't help us much when it comes to differentiating between letters that appear with roughly the same frequency (like \texttt{R} and \texttt{S} in this ciphertext).
If we think about English text for a bit, we notice that certain pairs of letters, called \emph{bigrams}, appear together more frequently than others (e.g. \texttt{q} is almost always followed by a \texttt{u} and \texttt{th} is a common pair).
Here are a few of the bigram frequencies from our ciphertext

\begin{center}
\begin{tabular}{c | c c c c c c c c c}
    &\texttt{W} & \texttt{B} & \texttt{R} & \texttt{S} & \texttt{I} & \texttt{V} & \texttt{A} & \texttt{P} & \texttt{N}\\
    \hline
    \texttt{W} & 3 & 4 & 12 & 2 & 4 & 10 & 14 & 3 & 1\\
    \texttt{B} & 4 & 4 & 0 & 11 & 5 & 5 & 2 & 4 & 20\\
    \texttt{R} & 5 & 5 & 0 & 1 & 1 & 5 & 0 & 3 & 0\\
    \texttt{S} & 1 & 0 & 5 & 0 & 1 & 3 & 5 & 2 & 0\\
    \texttt{I} & 1 & 8 & 10 & 1 & 0 & 2 & 3 & 0 & 0\\
    \texttt{V} & 8 & 10 & 0 & 0 & 2 & 2 & 0 & 3 & 1\\
    \texttt{A} & 7 & 3 & 4 & 2 & 5 & 4 & 0 & 1 & 0\\
    \texttt{P} & 0 & 8 & 6 & 0 & 1 & 1 & 4 & 0 & 0\\
    \texttt{N} & 14 & 3 & 0 & 1 & 1 & 1 & 0 & 7 & 0\\
\end{tabular}
\end{center}

That is, this table tells us that \texttt{WN} appears once and \texttt{NW} appears 14 times.
In English, the letter \texttt{h} frequently comes before \texttt{e} and rarely comes after it, so it's a safe guess that \texttt{h} maps to \texttt{N} in this particular substitution.
Since \texttt{th} is the most common digram in English and \texttt{BN} is the most common digram in the ciphertext, we guess that \texttt{t} maps to \texttt{B}.
Other features of the English language lead to more educated guesses that lead to a full cryptanalysis of the ciphertext.

\begin{problem}
    Finish decrypting the ciphertext. One place to start is by looking for vowels and noting that some vowels like \texttt{a}, \texttt{i} and \texttt{o} tend to avoid each other.
\end{problem}



\subsection{Divisibility and Greatest Common Divisors}
Some of the most widely-used cryptosystems today make heavy use of abstract algebra and number theory.
Roughly speaking, number theory is concerned with properties of the integers, $\Z$, like divisibility and solutions to equations with integer variables.

\begin{definition}
    Let $a$ and $b$ be integers with $b\neq 0$. We say that $b$ \emph{divides} $a$ if $a=bc$ for some integer $c$, in which case, we write $b\mid a$.
\end{definition}

\begin{example}
    \begin{enumerate}[(a)]
        \item We call the integers divisible by 2 \emph{even} and those that aren't \emph{odd}. Is zero even or odd?

        \item 713 is divisible by 23 since $713 = 23\cdot 31$. The numbers used in everyday cryptographic applications are hundreds or even thousands of digits long.

        \item A number $n$ is divisible by 5 if and only if it ends in a 0 or a 5 (when written in base 10, of course). To see this, write
        \[
            n = d_0 + 10d_1 + 10^2d_2 + \cdots + 10^kd_k,
        \]
        where $k\geq 0$ and $d_i \in \{0, 1, 2, \ldots, 9\}$ for all $i$.
        Then $d_0$ is the number that $n$ ``ends'' with, so if it's 0 or 5, we can just factor a 5 out of the right-hand side to see that $n$ is divisible by 5.
        Conversely, if we rearrange this,
        \[
            d_0 = n - 10d_1 - 10^2d_2 - \cdots - 10^kd_k,
        \]
        we see that if $n$ is divisible by 5, then the whole right-hand side (which is equal to $d_0$) is also divisible by 5.
    \end{enumerate}
\end{example}

We record some basic properties of divisibility here. The proof of this proposition is a straightforward exercise.

\begin{proposition}
    Let $a$, $b$ and $c$ be integers.
    \begin{enumerate}[(a)]
        \item If $a\mid b$ and $b\mid c$, then $a\mid c$.
        \item If $a\mid b$ and $b\mid c$, then $a = \pm b$.
        \item If $a\mid b$ and $a\mid c$, then $a\mid (b+c)$ and $a\mid (b-c)$.
    \end{enumerate}
\end{proposition}

\begin{question}
    For those familiar with equivalence relations, is divisibility an equivalence relation on $\Z$?
\end{question}

\begin{definition}
    A \emph{common divisor} of integers $a$ and $b$ is a positive integer $d$ that divides both of them.
    The \emph{greatest common divisor} of $a$ and $b$ is the largest positive integer $d$ such that $d\mid a$ and $d\mid b$ and we write $d = \gcd(a,b)$ or $d = (a,b)$ if there is no possibility of confusion.
\end{definition}

\begin{example}
    \begin{enumerate}[(a)]
        \item Find the greatest common divisor of $132$ and $66$ by listing out all of their divisors.

        \item Find the greatest common divisor of $80$ and $5$. Other than the number being pretty small, why was this easy to do? Prove your idea.
    \end{enumerate}
\end{example}

Of course given integers $a$ and $b$, it's not always the case that $a\mid b$ or $b\mid a$.
In this case, we get a (unique) remainder.

\begin{proposition}
    For any positive integers $a$ and $b$, there exist unique integers $q$ and $r$ such that
    \begin{equation}\label{division}
        a = bq + r\qquad \text{with }0\leq r < b.
    \end{equation}
    Here we call $q$ the \emph{quotient} and $r$ the \emph{remainder} when $a$ is divided by $b$.
\end{proposition}
\begin{proof}
    Homework exercise.
\end{proof}

Division with remainder provides us with a way of finding the gcd of two integers.
To see this, rearrange (\ref{division}) to obtain
\[
    r = a - bq.
\]
If $d$ is a common divisor of $a$ and $b$, then it clearly divides the right-hand side of this equation, so it must divide $r$ as well.
A similar rearrangement (which?) shows that if $c$ is a common divisor of $b$ and $r$, then it must also divide $a$.
We then have that the common divisors of $a$ and $b$ are the common divisors of $b$ and $r$, so we must have that
\[
    \gcd(a,b) = \gcd(b,r).
\]
This is great because if we assume that $a > b$, then we've reduced the problem of finding $\gcd(a,b)$ to finding the gcd of two smaller numbers, $b$ and $r$.
We can then repeat this: divide $b$ by $r$ to obtain
\[
    b = q'r + r',\qquad \text{with }0\leq r' < r.
\]
By the same reasoning, we have that
\[
    \gcd(a,b) = \gcd(b,r) = \gcd(r,r').
\]
Since the remainders are positive numbers that get strictly smaller after each division, we must eventually reach a remainder of zero. The remainder right before this one is then the gcd of $a$ and $b$.
\begin{example}\label{gcd ex}
    Let's compute $\gcd(12345, 11111)$.
    Even without a calculator it's sometimes easy to eyeball how many times one number goes into another.
    \begin{align*}
        12345 &= 11111\cdot 1 + 1234\\
        11111 &= 1234\cdot 9 + 5\\
        1234 &= 5\cdot 246 + 4\\
        5 &= 4\cdot 1 + 1\\
        4 &= 1\cdot 4 + 0
    \end{align*}
    The second-to-last remainder we found was 1, so we conclude that $\gcd(12345, 11111) = 1$.
    Note that even though the numbers involved started out somewhat large (for by-hand computations), we were able to calculate the gcd in just a few steps.
\end{example}

This procedure for computing the gcd of two integers is called the \emph{Euclidean algorithm} after the ancient Greek mathematician.
We summarize it here.
\begin{theorem}
    Let $a\geq b$ be positive integers.
    Then the following algorithm computes $\gcd(a,b)$ in a finite number of steps (i.e., the algorithm eventually terminates).

    \begin{algorithmic}[1]
        \State Let $r_0 = a$ and $r_1 = b$.
        \State Set $i = 1$. 
        \State Divide $r_{i-1}$ by $r_i$ with remainder to obtain quotient $q_i$ and remainder $r_{i+1}$.
        \[
            r_{i-1} = r_i\cdot q_i + r_{i+1},\qquad \text{with }0\leq r_{i+1}<r_i.
        \]
        \State If $r_{i+1} = 0$, then $r_i = \gcd(a,b)$ and the algorithm terminates.
        \State Otherwise, $r_{i+1} > 0$. Set $i = i+1$ and go to Step 3.
    \end{algorithmic}

\end{theorem}

How many times do we need to repeat the division step of the algorithm?
Let's start by looking at how much the remainders drop at each step.
At each step we have two possibilities: either $r_{i+1} \leq \frac{1}{2}r_i$ or $r_{i+1} > \frac{1}{2}r_i$.
In the first case, since the remainders are strictly decreasing, we have
\[
    r_{i+2} < r_{i+1} \leq \frac{1}{2}r_i.
\]
In the other case we must have $r_i = r_{i+1}\cdot 1 + r_{i+2}$. Rearranging, we have
\[
    r_{i+2} = r_i - r_{i+1} < r_i - \frac{1}{2}r_i = \frac{1}{2}r_i.
\]
In either case, we have that the remainder drops by at least half every two steps. After $2k+1$ steps we then have
\[
    r_{2k+1} < \frac{1}{2}r_{2k-1} < \frac{1}{2^2}r_{2k-3} < \cdots < \frac{1}{2^k}r_1 = \frac{1}{2^k}b.
\]
If $k$ is the smallest integer such that $b/2^k<1$, then we have $r_{2k+1} = 0$.
Setting $k = \lfloor \log_2 b\rfloor + 1$ does the trick.
The $gcd$ is then found on step at most $2k = 2\lfloor \log_2b\rfloor + 2$.

\begin{remark}
    Pretty much all cryptography software includes some implementation of the Euclidean algorithm.
    Computers store integers in their binary representations where an integer $N$ takes $n = \lfloor \log_2 N\rfloor +1$ bits of memory (why?). The above analysis shows that the Euclidean algorithm runs in a number of steps equal to at most twice the number of bits $(2n)$ it takes to store the smaller of its two inputs.
    When the number of steps it takes an algorithm to complete grows (at most) like a polynomial in its input size, then we consider it to be (reasonably) efficient.
\end{remark}

The Euclidean algorithm also gives us a way of writing $\gcd(a,b)$ as a linear combination of $a$ and $b$.

\begin{example}
    Let's return to Example \ref{gcd ex}.
    % We know $\gcd(12345, 11111) = 1$ since 1 is the second-to-last remainder.
    % Isolate the gcd and use the previous two equations to get the remainders $4$ and $5$ in terms of earlier remainders.
    % \begin{align*}
    %     1 &= 5 - 4\cdot 1\\
    %      &= (11111-1234\cdot 9) - (1234-5\cdot 246)\\
    %      &= 11111 -1234\cdot 10 + 5\cdot 246
    % \end{align*}
    % Now get the remainder 1234 in terms of the previous remainders and do the same for 5 once more.

    % \begin{align*}
    %     1 &= 11111 -1234\cdot 10 + 5\cdot 246\\
    %       &= 11111 - (12345 - 11111)\cdot 10 + (11111-1234\cdot 9)\cdot 246\\
    %       &= 12345\cdot (-10) + 11111\cdot 257 + 1234\cdot (2214)\\
    %       &= 
    % \end{align*}

    Write $a = 12345$ and $b = 11111$ and solve for the first remainder, 1234, in terms of $a$ and $b$:
    \[
        1234 = a - b.
    \]
    Now plug this into the second equation to get
    \[
        b = (a-b)\cdot 9 + 5,
    \]
    So the next remainder, 5, can be written in terms of $a$ and $b$ as
    \[
        5 = -9a + 10b.
    \]
    Plug this along with the expression for 1234 into the third equation to get
    \[
        a-b = (-9a + 10b)\cdot 246 + 4,
    \]
    which gives the next remainder, 4, in terms of $a$ and $b$:
    \[
        4 = 2215a - 2461b.
    \]
    Finally, plug the expressions for $4$ and $5$ into the second-to-last equation to get
    \[
        1 = (-9a + 10b) - (2215a - 2461b) = -2224a + 2471b.
    \]
\end{example}

This example is more or less a proof of the following theorem.

\begin{theorem}\label{bezout}
    Let $a$ and $b$ be positive integers.
    Then the equation
    \[
        ax + by = c
    \]
    has integer solutions for $x$ and $y$ if and only if $c$ is divisible by $\gcd(a,b)$.
    Moreover, if $(x_0, y_0)$ is a particular solution to this equation, then every other solution has the form
    \[
        x = x_0 + \frac{kb}{\gcd(a,b)},\quad y = y_0 - \frac{ka}{\gcd(a,b)}
    \]
    for some integer $k$.
\end{theorem}










\subsection{Modular Arithmetic}

Recall that when encrypting a message with a shift cipher with key $k$, each letter in the plaintext is shifted forward in the alphabet by $k$ positions.
Importantly, we \emph{wrap around} the alphabet if we shift past the letter \texttt{Z} (or whatever letter is at the end of the relevant alphabet).
This idea of wrapping around the end back to the beginning comes up in our day-to-day lives when we think about telling time.
Four hours after 9AM is 1PM since we \emph{wrap around} 12pm back to 1PM (the same idea holds if you prefer to think in military time - three hours after 2300 is 0200).
We'll look at this mathematically with the idea of \emph{congruence}.

\begin{definition}
    Let $m\geq 1$ be an integer. We say that the integers $a$ and $b$ are \emph{congruent modulo }$m$ if the difference $a-b$ is divisible by $m$.
    In this case we write
    \[
        a \equiv b \pmod{m}
    \]
    and call $m$ the \emph{modulus}.
\end{definition}


\begin{example}
    We have that $2 \equiv 5\pmod{7}$. We also have that $2\equiv 9\pmod 7$ and $2 \equiv 16\pmod 7$.
\end{example}

Importantly, congruences respect familiar operations like addition and multiplication, but are a little trickier when it comes to division.

\begin{proposition}\label{well defined}
    Let $m\geq 1$ be an integer.
    \begin{enumerate}
        \item If $a_1 \equiv a_2 \pmod m$ and $b_1 \equiv b_2\pmod m$, then
        \[
            a_1 \pm b_1 \equiv a_2 \pm b_2 \pmod m\qquad \text{and}\qquad a_1b_1\equiv a_2b_2\pmod m.
        \]

        \item Let $a$ be an integer. Then there exists an integer $b$ such that
        \[
            ab \equiv 1\pmod m
        \]
        if and only if $\gcd(a,m) = 1$.
        In this case, we call $b$ the \emph{multiplicative inverse of $a$ modulo $m$} and we write $b = a^{-1}\pmod m$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    The proof of part (a) isn't super interesting, so we'll skip it.

    For part (b), first suppose that $\gcd(a,m) = 1$.
    Then by Theorem (\ref{bezout}), we can find $u$ and $v$ such that $au+mv = 1$.
    But if we rearrange this, we have
    \[
        au - 1 = mv,
    \]
    so the difference $au-1$ is divisible by $m$ and $au\equiv 1\pmod m$.
    In this case, $u$ is an inverse of $a\pmod m$.

    On the other hand, suppose $a$ has a multiplicative inverse $b\pmod m$.
    Then $m$ divides the difference $ab-1$ so we have
    \[
        ab - km = 1
    \]
    for some integer $k$.
    If $d$ is some (positive) common divisor of $a$ and $m$, then $d$ must divide the left-hand side of this equation.
    But then $d$ must divide 1, so we must have $d = 1$.
    It must then be the case that $\gcd(a, m) = 1$.
\end{proof}

Part (b) of this proposition gives us a partial analogue of division modulo $m$.
Just like how the rational number $1/2$ has the property that $(1/2)\cdot 2 = 1$, the number $3$ has the property that $3\cdot 2 = 6 \equiv 1\pmod 5$, so $3$ plays a similar role to $1/2$.
What's more is that our proof of part (b) gives us an algorithm for computing the modular inverse: the extended Euclidean algorithm.

\begin{example}
    Let's find the inverse of $4$ modulo 7 (if it exists at all).
    First compute $\gcd(4, 7)$.
    \begin{align*}
        7 &= 4\cdot 1 + 3\\
        4 &= 3\cdot 1 + 1\\
        3 &= 1\cdot 3 + 0
    \end{align*}
    So $\gcd(4, 7) = 1$, so we know a modular inverse exists.
    We find it by substituting in expressions for the remainders.

    \begin{align*}
        1 &= 4 - 3\cdot 1\\
          &= 4 - (7-4)\\
          &= 4\cdot 2 - 7.
    \end{align*}
    Rearranging this, we see that $4\cdot 2 - 1 = 7$, so $4\cdot 2\equiv 1\pmod 7$, and 2 is the inverse of 4 modulo 7.
\end{example}

Remember that the Euclidean algorithm is really efficient (for a computer at least - so is the extended one), so finding inverses is efficient as well.

Returning to the above example, note that $4\cdot 9 = 36 \equiv 1\pmod 7$ as well, so we can just as easily say that $9$ is an inverse of 4 modulo 7.
It would be nice if there was just one inverse or a way for two people to pick the same inverse every time.
Division with remainder gives us a way of doing this.
If $b$ is an inverse of $a$ modulo $m$, write
\[
    b = mq + r\quad\text{with }0\leq r < m.
\]
Then $r$ is always between 0 and $m-1$. Since this $r$ is unique, we can agree that we always work with the integers 0 through $m-1$ when we work modulo $m$.
This idea is encapsulated in the following proposition.

\begin{proposition}\label{remainders only}
    The integers $a$ and $b$ are congruent modulo $m$ if and only if they have the same remainder when divided by $m$.
\end{proposition}

Recall the notion of equivalence relations.

\begin{definition}
    A relation $\sim$ on a set $X$ is an \emph{equivalence relation} if the following all hold.
    \begin{enumerate}
        \item (Reflexivity) $x\sim x$ for all $x\in X$.
        \item (Symmetry) $x\sim y$ if and only if $y\sim x$ for any $x,y\in X$.
        \item (Transitivity) if $x\sim y$ and $y\sim z$ then $x\sim z$.
    \end{enumerate}
    For each $x\in X$, the \emph{equivalence class of }$x$, denoted $[x]$ (or sometimes $\overline{x}$) is
    \[
        [x] = \{y\in X: x\sim y\}.
    \]
    We can form the new set $X/\sim$, the \emph{quotient of $X$ by $\sim$} by just taking equivalence classes.
    \[
        X/\sim = \{[x]: x\in X\}.
    \]
\end{definition}

Modular arithmetic is a concrete example of this.

\begin{proposition}
    Fix a positive integer $m\geq 2$. Then equivalence modulo $m$ is an equivalence relation on $\Z$.
\end{proposition}

Moreover, Proposition (\ref{remainders only}) leads us to think that the quotient of $\Z$ by ``equivalence modulo $m$'' is the ``correct'' object to work with and to choose our equivalence classes to be $[0], \ldots, [m-1]$.

\begin{definition}
    The set $\Z/m\Z$ is defined to be the set of integers quotiented by the relation ``equivalent modulo $m$''. Specifically,
    \[
        \Z/m\Z = \{[0], \ldots, [m-1]\},
    \]
    where $[a] = \{b\in \Z: a\equiv b\pmod m\}$.
\end{definition}
\begin{remark}
    When working with $\Z/m\Z$, we usually drop the $[\cdot ]$ when talking about its elements, which are equivalence classes. That is, it technically doesn't make sense to write $2\in \Z/5\Z$ since 2 isn't an equivalence class. However, as the next proposition shows, the equivalence class $[2]$ can be made to behave a lot like the ordinary integer 2.
\end{remark}

We can carry the notions of addition and multiplication over to the quotient as well.

\begin{definition}
    For $[a], [b]\in \Z/m\Z$, define $[a] + [b]$ to be $[a+b]$ and $[a]\cdot [b]$ to be $[ab]$.
\end{definition}

\begin{remark}
    Technically, the above definition should be made into a proposition that says this definition is \emph{well-defined}.
    That is, we need to show that if $a\equiv a'$ and $b\equiv b'$ then we want $[a] + [b] = [a'] + [b']$ and $[a]\cdot [b] = [a']\cdot [b']$.
    This follows easily from Proposition (\ref{well defined}).
\end{remark}

Let's think about Theorem \ref{bezout} for a bit in this context by looking at equations in $\Z/m\Z$.

\begin{example}
    \begin{enumerate}
        \item Does $2x\equiv 3$ have a solution modulo 5?
        It would be nice if we could ``divide by 2'' and that's exactly what a multiplicative inverse lets us do.
        It's easy to verify that $4$ is the inverse of $2\pmod 5$, so multiplying both sides of this equation through by 4 gives $x \equiv 12 \equiv 2\pmod 5$.

        \item What about $2x\equiv 3\pmod 6$?
        This equation in $\Z/6\Z$ is equivalent to the integer equation $2x - 3 = 6y$,
        which has no solution since the left-hand side is always odd while the right-hand side is always even.
        Another way we can think about it is that we can't ``divide by 2'' since $\gcd(2, 6) = 2 \neq 1$.

        \item What about $2x \equiv 4 \pmod 6$?
        Just like in the last example, we can't divide by 2.
        However, it's easy to see that $x \equiv 2\pmod 6$ is a solution.
        But this solution isn't unique since $x \equiv 5 \pmod 6$ is also a solution.
    \end{enumerate}
\end{example}

In short, the existence of an inverse, as determined by Theorem \ref{bezout} determines whether or not equations like $ax \equiv b\pmod m$ have solutions.
If $\gcd(a, m) = 1$, then there's a unique solution.
Otherwise, there can either be no solution or there might be multiple solutions.
If we want to restrict ourselves to the (equivalence classes of) integers that \emph{do} have inverses modulo $m$, then we use the following object.

\begin{definition}
    Fix an integer $m\geq 2$. Then the set of \emph{units modulo $m$} is denoted by
    \begin{align*}
        (\Z/m\Z)^\times &= \{a\in \Z/m\Z: \gcd(a,m) = 1\}\\
        &= \{a\in \Z/m\Z: a\text{ has an inverse modulo }m\}.
    \end{align*}
\end{definition}










\subsection{Prime Numbers, Unique Factorization, and Finite Fields}

The ``building blocks'' of the integers are the prime numbers.
\begin{definition}
    An integer $p$ is called \emph{prime} if $p\geq 2$ and if the only positive integers dividing $p$ are 1 and $p$.
\end{definition}

Note that if $p$ is prime, then $\gcd(a,p) = 1$ for each $1\leq a < p$ (why?).
Consequently, each nonzero element of $\Z/p\Z$ has a multiplicative inverse, i.e.
\[
    (\Z/p\Z)^\times = \{1, 2, \ldots, p-1\}.
\]
The set $\Z/p\Z$ forms a structure that we call a (finite) \emph{field}: a set where we can add and subtract as well as multiply and divide by (nonzero) elements.
We denote this field by $\F_p$.
Other examples of fields include $\R$ and $\Q$ but not $\Z$.

\begin{proposition}\label{prime divides product}
    Let $p$ be a prime number and suppose that $p \mid ab$.
    Then $p\mid a$ or $p\mid b$.
    More generally, if
    \[
        p\mid a_1a_2\cdots a_k,
    \]
    then $p\mid a_i$ for some $i$.
\end{proposition}
\begin{proof}
    We'll prove the first statement and you'll prove the second one in discussion.
    If $p$ divides $a$ then we're done.
    If $p\nmid a$, then $\gcd(a,p) = 1$ (why?), so we can write
    \[
        au + pv = 1
    \]
    for some integers $u$ and $v$.
    Multiplying this through by $b$ gives
    \[
        abu + pbv = b.
    \]
    By assumption, $p\mid ab$ and clearly $p\mid pbv$, so $p$ divides the left-hand side of this equation. Consequently, $p$ divides the right-hand side, which is $b$.
\end{proof}

Using this, we can prove what we said earlier about primes being ``building blocks.''

\begin{theorem}[The Fundamental Theorem of Arithmetic]
    Let $a\geq 2$ be an integer. Then $a$ can be factored as a product of prime numbers
    \begin{equation}\label{factorization}
        a = p_1^{e_1}p_2^{e_2}\cdots p_r^{e_r}
    \end{equation}
    for some positive integer $r$.
    Furthermore, this factorization is unique up to rearrangement of the primes.
\end{theorem}
\begin{proof}
    We prove that we can factor into primes by induction and uniqueness will come later.
    Our base case is $a=2$, and this itself is a prime factorization since 2 is prime.
    Suppose then that every integer less than $a$ can be factored into primes.
    If $a$ itself is prime, then we're done by the same reasoning we used in the base case.
    Otherwise, $a = bc$ where $1 < b,c < a$.
    By the induction hypothesis, we can factor $b$ and $c$ into primes:
    \[
        b = p_1^{e_1}\cdots p_k^{e_k},\quad c= q_1^{f_1}\cdots q_\ell^{f_\ell}.
    \]
    But then $a = p_1^{e_1}\cdots p_k^{e_k}q_1^{f_1}\cdots q_\ell^{f_\ell}$ is a factorization of $a$.
    You'll prove the uniqueness part of this statement in discussion.
\end{proof}

Looking at the factorization of $a$ into primes (\ref{factorization}), we call the number of times a particular prime, $p$, appears in the factorization the \emph{order of $p$ in $a$} and denote it by $ord_p(a)$.
That is, in the factorization (\ref{factorization}), $ord_{p_i}(a) = e_i$.











\subsection{Powers and Primitive Roots in Finite Fields}


We can add, subtract, multiply and (sometimes) divide by elements of $\Z/m\Z$.
Since we can multiply, we can definitely exponentiate in exactly the way you think we would.
If $a\in \Z/m\Z$ and $k$ is a nonnegative integer, then $a^k$ is the product of $a$ with itself $k$ times, taken modulo $k$.
Moreover, if $a$ has inverse $a^{-1}$, then we can define negative powers of $a$ as positive powers of $a^{-1}$.
We need to be a little careful though.
We can raise an element of $\Z/m\Z$ to an integer power, but it doesn't make sense to raise an element of $\Z/m\Z$ to the power of another element of $\Z/m\Z$.

\begin{example}
    We clearly have that $2^1 \equiv 2\pmod 5$.
    However, $2^5 = 32\equiv 2\pmod 5$ even though $5 \not\equiv 1\pmod 5$.
    In other words, $a^b \equiv a^c \pmod m$ \emph{does not} imply that $b \equiv c\pmod m$.
\end{example}

A few of the main cryptographic protocols we'll talk about come from the properties of modular exponentiation, so let's talk a bit about it.
Let's look at some of the powers of 2 modulo 7
\[
    2^1 \equiv 2,\ 2^2 \equiv 4,\ 2^3 = 8 \equiv 1,\ 2^4 = 16 \equiv 2,\ 2^5 = 32 \equiv 4,\ 2^6 = 64 \equiv 1, \ldots
\]
It looks like we get a repeating pattern of 1, 2, 4.
In fact, we can prove that this pattern holds true: take any positive integer $k$ and divide it by 3 with remainder to get $k = 3q + r$. Then
\[
    2^k = 2^{3q + r} = (2^3)^q\cdot 2^r \equiv 1^q\cdot 2^r \equiv 2^r\pmod 7.
\]
That is, the value of $2^k$ only depends on the remainder we get when we divide $k$ by 3, i.e. we care about what $k$ is modulo 3, \emph{not} modulo 7.
What happens with the powers of other numbers, say 3 (still modulo 7)?
\[
    3^1 = 3,\ 3^2 \equiv 2,\ 3^3\equiv 6,\ 3^4 \equiv 4,\ 3^5\equiv 6,\ 3^6\equiv 1,\ 3^7\equiv 3,\ldots
\]
Like with 2, we have that $3^6 \equiv 1\pmod 7$.
However, it looks like we get a repeating pattern of length six this time.
What's more is that the powers of 3 give us all the nonzero elements of $\Z/7\Z$.

Let's solidify the first of these observations into a theorem.
\begin{theorem}[Fermat's Little Theorem]
    Let $p$ be a prime number and let $a$ be any integer. Then
    \[
        a^{p-1} \equiv \begin{cases}
            1\pmod p,&\text{if }p\nmid a,\\
            0\pmod p,&\text{if }p\mid a.
        \end{cases}
    \]
\end{theorem}
\begin{proof}
    If $p$ divides $a$ then it divides every power of $a$, so let's just look at the case where $p\nmid a$.
    Let's look at the numbers
    \begin{equation}\label{distinct}
        a,\ 2a,\ 3a,\ \ldots,\ (p-1)a,
    \end{equation}
    We claim that these are all \emph{distinct} when reduced modulo $p$.
    Indeed, if $ka \equiv ja\pmod p$, then $p$ divides $a(k-j)$.
    By Proposition \ref{prime divides product}, $p$ must then divide $a$ or $k-j$.
    Since we've assumed that $p\nmid a$, we must have that $p$ divides $k-j$.
    But we haven't listed any multiples of $p$ above, so we must have $k=j$.

    Now let's multiply all the elements in (\ref{distinct}) together.
    On one hand, this is clearly $a^{p-1}\cdot (p-1)!$.
    On the other hand, since since these are $p-1$ distinct nonzero integers between 1 and $p-1$, the must be all of the integers in this range, so their product is $(p-1)!$.
    We must then have
    \[
        a^{p-1}\cdot (p-1)! \equiv (p-1)!\pmod p.
    \]

    We can then cancel the $(p-1)!$ from both sides (why?) to obtain $a^{p-1}\equiv 1\pmod p$.
\end{proof}

This theorem has some really powerful implications for computation.

\begin{example}
    The integer $p = 15485863$ is prime, so by Fermat's little theorem we have
    \[
        2^{15485862} \equiv 1\pmod {15485863}.
    \]
    Even though the numbers involved are large ($2^{15485862}$ has more than 400,000 digits), we can write the above identity without doing any real computation (we had to know that 15485863 is prime first, and we'll see some good algorithms for verifying this later).
\end{example}

Fermat's little theorem tells us that if $p\nmid a$, then $a^{p-1}\equiv 1\pmod p$, but as we saw with the powers of 2 modulo 7, a smaller power of $a$ might be congruent to 1 modulo $p$.
This motivates the following definition.

\begin{definition}
    Let $p\geq 2$ be prime. For any integer $a$ such that $p\nmid a$, the \emph{order of $a$ modulo $p$} is the smallest positive integer $k$ such that $a^k\equiv 1\pmod p$.
\end{definition}

Fermat's little theorem tells us that that the order of $a$ is at most $p-1$ so long as $p\nmid a$.
The following proposition claims that the order of $a$ modulo $p$ can't be just anything however.

\begin{proposition}
    Let $p$ be a prime and let $a$ be an integer with $p\nmid a$.
    If $a^n\equiv 1\pmod p$, then the order of $a$ modulo $p$ divides $n$.
    In particular, the order of $a$ divides $p-1$.
\end{proposition}

\begin{proof}
    Suppose $k$ is the order of $a$ modulo $p$.
    We divide $n$ by $k$ to obtain
    \[
        n = kq + r
    \]
    for some $0\leq r < k$.
    We then have
    \[
        1 \equiv a^n \equiv a^{kq + r} \equiv (a^k)^q\cdot a^r \equiv 1^q\cdot a^r \equiv a^r\pmod p.
    \]
    But $k$ is the smallest positive power of $a$ congruent to 1 modulo $p$, so we must have $r = 0$ and $k\mid n$.
\end{proof}

Looking at the powers of 3 modulo 7, we see that sometimes the powers of $a$ modulo $p$ can fill out all of the nonzero residues modulo $p$.
The following theorem says that there is always such a $p$ and you'll prove it on your next homework assignment.
\begin{theorem}[Primitive Root Theorem]
    Let $p$ be prime.
    Then there exists an element $g \in \F_p^\times$ such that
    \[
        \F_p^\times = \{1, g, g^2, \ldots, g^{p-2}\}.
    \]
    Such a $g$ is called a \emph{generator} or \emph{primitive root of }$\F_p$.
\end{theorem}


Let's talk a little about how to actually compute $a^k\pmod m$.
The naive way, repeated multiplication, would simply compute $a^i$ for all $i\leq k$:
\[
    a_1 \equiv a\pmod m,\quad a_2 \equiv a\cdot a_1 \pmod m,\quad a_3 \equiv a\cdot a_2\pmod m,\quad \ldots\quad a_k = a\cdot a_{k-1}\pmod m.
\]
If $k$ is of moderate size (for a computer), say around 1000 bits (around 300 digits), then the time it would take to complete this algorithm would be greater than the estimated age of the universe, even if you reduced modulo $m$ after each step (if you didn't, then the integer $a^k$ would take up more bits than there are particles in the universe by some estimates).
Let's look at an example for how to compute large powers very efficiently.

\begin{example}
    Let's compute $3^{75}\pmod {100}$.
    We start by writing $75$ in binary.
    The largest power of 2 that is no larger than 75 is 64, so
    \[
        75 = 64 + 11.
    \]
    Now the largest power of 2 at most 11 is 8. We repeat this process.
    \begin{align*}
        76 &= 64 + 8 + 3\\
        &= 64 + 8 + 2 + 1.
    \end{align*}
    Using this we can write
    \begin{equation}\label{3 to binary}
        3^{76} = 3^{64 + 8 + 2 + 1} = 3^{64}\cdot 3^8\cdot 3^2\cdot 3^1.
    \end{equation}
    Now we can compute the seven numbers
    \[
        3,\ 3^2,\ 3^4,\ 3^8,\ \ldots,\ 3^{64}
    \]
    modulo 100 quite easily - each number is just the square of the one before it.
    Now using (\ref{3 to binary}), we decide which of these powers of 3 to multiply together.
    \begin{center}
    \begin{tabular}{|c|| c | c | c | c | c | c | c | c|}
        \hline
        $i$ & 0 & 1 & 2 & 3 & 4 & 5 & 6\\
        \hline
        $3^{2^i}\pmod{100}$ & 3 & 9 &  81 & 61 & 21 & 41 & 81\\
        \hline
    \end{tabular}
    \end{center}
    This gives
    \begin{align*}
         3^{76} &= 3^1\cdot 3^2\cdot 3^8\cdot 3^{64}\\
         &\equiv 3\cdot 9\cdot 61\cdot 81 \pmod{100}\\
         &\equiv 21 \pmod{100}.
    \end{align*}
\end{example}


Let's describe this algorithm, sometimes called the \emph{fast powering algorithm} or the \emph{square-and-multiply algorithm}, more formally.

\begin{enumerate}
\item \textbf{Given: }integers $g$, $A$ and $N$
\item Compute the binary expansion of $A$ as 
\[
    A = A_0 + A_1\cdot 2 + A_2\cdot 2^2 + \cdots + A_r\cdot 2^r,\quad \text{with }A_i\in \{0,1\}\text{ for all }i.
\]
\item Compute the powers $A^{2^i}\pmod m$ for each $0\leq i \leq r$ by squaring.
    \begin{align*}
        a_0 &\equiv g\pmod N\\
        a_1 &\equiv a_0^2\equiv g^2\pmod N\\
        a_2 &\equiv a_1^2\equiv g^{2^2}\pmod N\\
        \vdots\\
        a_r &\equiv a_{r-1}^2\equiv g^{2^r}\pmod N.
    \end{align*}

\item Compute $g^A\mod N$ by multiplication.
    \begin{align*}
        g^A &= g^{A_0 + A_1\cdot 2 + A_2 \cdot 2^2 + \cdots + A_r\cdot 2^r}\\
        &= g^{A_0}\cdot (g^2)^{A_1}\cdot (g^{2^2})^{A_2}\cdots (g^{2^r})^{A_r}\\
        &\equiv a_0^{A_0}\cdot a_1^{A_1} \cdots a_r^{A_r}\pmod n.
    \end{align*}
\end{enumerate}








\addtocounter{subsection}{1}
\subsection{Symmetric and Asymmetric Ciphers}
Let's briefly formalize some notions from cryptography.
Suppose Alice and Bob agree to send and receive messages using some cipher.
If the same key is used for encryption and decryption, then we say that the cipher is \emph{symmetric}.
We can describe a symmetric cipher with these sets and maps.
\begin{align*}
    \mathcal{K} &= \text{all possible keys}\\
    \mathcal{M} &= \text{all possible plaintexts}\\
    \mathcal{C} &= \text{all possible ciphertexts}\\
    e &: \mathcal{K}\times \mathcal{M}\to \mathcal{C}\\
    d &: \mathcal{K}\times \mathcal{C}\to \mathcal{M}.
\end{align*}
The function $e$ is the \emph{encryption function} that takes in a key, plaintext pair $(k, m)$ and outputs the corresponding ciphertext.
Likewise, the \emph{decryption function} $d$ takes a key, ciphertext pair $(k, c)$ and outputs the plaintext.

Of course we want decryption to undo encryption with the same key, so we require the following consistency condition.
\[
    d(k, e(k, m)) = m,\qquad \text{for all }k\in \mathcal{K},\ m\in \mathcal{M}.
\]
Another way to think about this condition is to define encryption and decryption functions $e_k$, $d_k$ for each key $k$,
\[
    e_k(\cdot) = e(k, \cdot),\qquad d_k(\cdot) = d(k, \cdot)
\]
Then the consistency condition just says that $d_k$ is the inverse function of $e_k$.
This of course requires that $e_k$ be injective.
\begin{example}
    The substitution cipher is a symmetric cipher.
\end{example}

When assessing the security of a cipher, we \emph{always assume that an eavesdropper, Eve, knows the method of encryption.}
That is, we assume Eve knows $e$ and $d$, but not the key $k$.
Under this assumption, here are some of the properties we want for $(\mathcal{K}, \mathcal{M}, \mathcal{C}, e, d)$.
\begin{enumerate}
    \item Easy encryption: for any $k\in \mathcal K$ and $m\in \mathcal M$, it's easy to compute $e_k(m)$.
    \item Easy decryption: for any $k\in \mathcal K$ and $c \in \mathcal C$, it's easy to compute $d_k(c)$.
    \item Strength against ciphertext-only attacks: If Eve knows that the ciphertexts $c_1, c_2, \ldots, c_n\in \mathcal C$ were encrypted using the same (unknown) key $k$, it should be hard for her to compute any of the corresponding plaintexts, $d_k(c_1), \ldots, d_k(c_n)$ without knowing $k$.

    \item Strength against known-plaintext attacks: If Eve knows some plaintexts and their corresponding ciphertexts $(m_1, c_1), \ldots, (m_n, c_n)$, where $c_i = e_k(m_i)$, then it should be hard for her to compute $d_k(c)$ for any new ciphertext $c$ without knowing $k$.

    \item Strength against chosen-plaintext attacks: If Eve \emph{chooses} some plaintexts $m_1, \ldots, m_n$ and knows their corresponding ciphertexts $c_1, \ldots, c_n$, it should still be hard for her to compute $d_k(c)$ for any new ciphertext $c$ without knowing $k$.

    \item Strength against chosen-ciphertext attacks: If Eve chooses some ciphertexts $c_1, \ldots, c_n$ and knows their corresponding plaintexts $m_1, \ldots, m_n$, it should still be hard for her to compute $d_k(c)$ for any new ciphertext $c$ without knowing $k$.
\end{enumerate}

\begin{example}
    The substitution cipher isn't very resistant to ciphertext-only attacks since it's vulnerable to frequency analysis when we have a long ciphertext.
    It's also quite vulnerable to known-plaintext attacks since we can just query a few messages to learn most of the substitutions.
\end{example}

\begin{remark}
    When we think of plaintexts we often think of strings of Latin characters.
    However, we can usually \emph{encode} written characters into numbers (indeed, this is what computers do), so we can safely think of $\mathcal M$ and $\mathcal C$ as sets of numbers or elements of $\Z/m\Z$.
\end{remark}

\begin{example}
    Let $\mathcal M = \mathcal C = \mathcal K = \F_p^\times$.
    For any key $k$, define encryption by
    \[
        e_k(m) = m\cdot k\pmod p.
    \]
    We can assume that basic arithmetic like addition and multiplication modulo $p$ is efficient to compute.
    We can also efficiently decrypt, since the Euclidean algorithm gives us a way to compute $k^{-1}\pmod p$ in around $2\log_2 p$ steps, so
    \[
        d_k(c) = k^{-1}\cdot c \pmod p
    \]
    is efficient to compute.
    This cipher is \emph{completely broken} (that is, we can learn the key) by a known plaintext attack.
    If we know $c$ and $m$, then we can simply compute
    \[
        k \equiv c^{-1}\cdot m\pmod p
    \]
    since we can efficiently compute $c^{-1}\pmod p$.
\end{example}

One issue with symmetric ciphers is how Alice and Bob agree on a key.
If they have to exchange the key in the open, then it's vulnerable to Eve.
Alternatively, they can meet in person, but this can be inconvenient or impossible.
This motivates the idea of \emph{public-key cryptography}.
Here, we have two separate keys, $k = (k_{priv}, k_{pub})$.
The \emph{public key}, $k_{pub}$ is known to everyone, while the \emph{private key} is known only to the person receiving messages.
We have encryption and decryption functions
\begin{align*}
    e_{k_{pub}} &: \mathcal M\to \mathcal C\\
    d_{k_{priv}} &: \mathcal C\to \mathcal M,
\end{align*}
that are inverse to one another.
It should be difficult to compute $d_{k_{priv}}$ without knowledge of $k_{priv}$ even if one knows $k_{pub}$.

If Bob wants to send Alice a message, then Alice first sends Bob her public key, $k_{pub}$ (or maybe she publishes it in some public place like some website).
Note that we should then assume that Eve knows $k_{pub}$ as well.
Bob then computes $e_{k_{pub}}(m)$ and sends it to Alice.
Alice then decrypts this using $k_{priv}$, which only she knows.
The private key $k_{priv}$ is sometimes called \emph{trapdoor information} for the \emph{one-way} function $e_{k_{pub}}$ because it should be hard to invert this function without knowledge of $k_{priv}$.









\section{Discrete Logarithms and Diffie-Hellman}
\addtocounter{subsection}{1}
\subsection{The Discrete Logarithm Problem}
Public-key cryptography revolves around functions that are easy to compute but hard to invert without some special trapdoor information.
We've seen that exponentiation modulo $p$ is easy to compute by the square-and-multiply algorithm and invertible by the primitive root theorem.
It turns out that it's also hard to invert.

\begin{definition}
    Let $g$ be a primitive root of $\F_p^\times$ and let $h\neq 0$ in $\F_p$.
    The \emph{discrete logarithm problem (DLP)} is the problem of finding an exponent $x$ such that
    \[
        g^x\equiv h\pmod p.
    \]
    The number $x$ is called the \emph{discrete logarithm of $h$ to the base $g$} and is defined modulo $p-1$ by Fermat's little theorem.
\end{definition}

Remember that since $g$ is a primitive root we have that
\[
    \F_p^\times = \{g^0, g^1, \ldots, g^{p-2}\},
\]
so $h \equiv g^i$ for some $0 \leq i \leq p-2$.
By Fermat's little theorem we have that $g^{p-1}\equiv 1\pmod p$, so
\[
    g^{i + k(p-1)} \equiv h\pmod p
\]
for any integer $k$.
This\footnote{If you've taken complex analysis, this might remind you of the fact that $e^{r + 2\pi i} = e^r$ in $\C^\times$ and why we have to take so-called \emph{branch cuts} of the logarithm in that setting.} is why we say that the discrete logarithm is defined modulo $p-1$.
In particular, there are infinitely many integers $x$ such that $g^x\equiv h\pmod p$ and the DLP asks us to find just one such $x$.
This motivates us to define the function
\[
    \log_g: \F_p^\times \to \Z/(p-1)\Z
\]
for any primitive root $g$.

It makes sense to call this function a logarithm because it behaves how you would hope a logarithm does.
\begin{proposition}
    For any primitive root $g$ of $\F_p^\times$ and any $a,b\in \F_p^\times$ we have that
    \[
        \log_g(ab) \equiv \log_g(a) + \log_g(b)\pmod{p-1}.
    \]
\end{proposition}
\begin{proof}
    You'll prove this in your discussion section.
\end{proof}

How do we compute discrete logarithms?
Let's look at the brute-force solution.
For any $A\in \Z/(p-1)\Z$ we can compute $g^A\pmod p$ in around $2\log_2p$ multiplications (steps), and if try to solve the DLP by trial and error, we'd compute
\[
    g^0,\ g^1,\ g^2, \ldots
\]
modulo $p$ until we find $h$.
If $h \equiv g^{p-2}$ then this will take around $2p\log_2p$ steps.
If $p$ is even moderately large, say at least $2^{100}$, then this will take way too many steps for even modern computers.

\begin{remark}
    In some ways exponentiation and logarithms have different properties when we're working in $\F_p$ instead of $\R$ or $\C$.
    Even though they behave the same way \emph{algebraically} (i.e. they follow the same ``rules''), exponentiation modulo $p$ appears to behaves ``randomly'' in its input (of course it isn't actually random, but if you were to graph the function $x\mapsto g^x \pmod p$ for a primitive root $g$, then you'll get a random-looking cloud of points).
\end{remark}

\begin{remark}
    We can talk about the DLP even if $g$ isn't a primitive root modulo $p$.
    If it isn't, then the DLP asks us to find $x$ such that $g^x\equiv h\pmod p$ if such an $x$ exists.

    In fact, we can talk about the DLP in the setting of an arbitrary group.
    If $G$ is a group (written multiplicatively), then the DLP for $G$ asks us to find, for any two given $g$ and $h$ in $G$, an integer satisfying
    \[
        \underbrace{g\cdot g \cdot \cdots \cdot g}_{x\text{ times}} = h.
    \]
\end{remark}













\subsection{Diffie-Hellman Key Exchange}
Recall that with a symmetric cipher, Alice and Bob need to both have a key $k$ that they use to encrypt or decrypt messages.
Even if the cipher that they use is extremely resistant to any attack you can think of, if they exchange the key in an insecure way, then their communications are as good as compromised.
Whitfield Diffie and Martin Hellman released a paper in 1976 showing a way of doing this with modular exponentiation, but it's since been claimed that the British GCHQ knew of this idea since the sixties and kept it secret.

The following protocol allows for Alice and Bob to establish a shared secret even in the presence of an adversary, Eve.
The idea is that they can then use this secret as a key (or somehow use it to make a key) for a symmetric cipher.

\begin{enumerate}
    \item (Public) Alice and Bob both publicly agree on a large (around $2^{1000}$) prime $p$ and an integer $g$ relatively prime to $p$.
    Usually, $g$ is a primitive root modulo $p$, but this isn't necessary.

    \item (Private) Alice chooses a secret integer $a$ and computes $A \equiv g^a\pmod p$.
    Likewise, Bob chooses a secret integer $b$ and computes $B \equiv g^b\pmod p$.
    They perform this part completely independently of each other.
    Alice and Bob can complete this step efficiently with the square-and-multiply algorithm.

    \item (Public) Alice sends $A$ to Bob and Bob sends $B$ to Alice.
    This happens publicly (sometimes said ``in the clear''), so Eve can see this.

    \item (Private) Alice uses her secret integer to compute $B^a \pmod p$ and Bob likewise computes $A^b\pmod p$.
    Again, this can be done efficiently with square-and-multiply.
\end{enumerate}

After these steps are completed, Alice and Bob share the following value
\begin{align*}
    A^b &\equiv (g^a)^b \pmod p\\
    &\equiv (g^b)^a\pmod p\\
    &\equiv B^a\pmod p.
\end{align*}

Eve has access to anything that was shared publicly, namely $p$, $g$, $A$, and $B$.
If Eve knew either $a$ or $b$, then she could simply compute $B^a$ or $A^b$ modulo $p$ to compromise the shared secret.
But in order for her to do this, she needs to solve either of the equations
\[
    g^x \equiv A\pmod p,\qquad g^y\equiv B\pmod p.
\]
That is, if Eve can solve the discrete logarithm problem, then she can find the shared secret.

\begin{example}
    Suppose Alice and Bob agree on the prime $p=13$ and the integer $g = 2$.
    Next, say Alice randomly chooses $a=7$ and Bob randomly (and independently) chooses $b = 9$.
    Alice computes $A\equiv g^a = 2^7 \equiv 11\pmod{13}$ and Bob computes $B\equiv g^b = 2^9 \equiv 5\pmod {13}$.
    Alice and Bob then exchange $A$ and $B$ in the clear and compute
    \[
        B^a = 5^7\equiv 8\pmod {13},\qquad A^b = 11^9\equiv 8\pmod {13}.
    \]
    If Eve can find $x$ such that $2^x\equiv 11\pmod {13}$ or $y$ such that $2^y\equiv 5\pmod{13}$, then she too can perform one of the above computations to arrive at the shared value of 8.
\end{example}

If Even can solve the DLP, then she can compromise Diffie-Hellman key exchange.
Does she really have to do this though?
Really, what she needs to do is solve the following problem.

\begin{definition}
    Let $p$ be prime and let $g\in \F_p^\times$, if for any $a$ and $b$ we can use $g^a\pmod p$ and $g^b\pmod p$ to compute $g^{ab}\pmod p$, then we can solve the \emph{Diffie-Hellman (search) problem} or \emph{DHP}.
\end{definition}

\begin{remark}
    The Diffie-Hellman problem doesn't actually require us to find either of the exponents $a$, $b$.
\end{remark}
Clearly DHP is no harder than DLP, but it's \emph{unknown} whether or not solving the DHP would allow one to solve the DLP as well.

\begin{thebibliography}{13}

\bibitem{HPS} Hoffstein, Jeffrey, Jill Pipher and Joseph H. Silverman. \href{https://link.springer.com/book/10.1007/978-1-4939-1711-2}{\textit{An Introduction to Mathematical Cryptography}}. Second Edition. Springer New York, NY. 2014.
% \bibitem{Grav} Gravner, Janko. Online lecture notes, https://www.math.ucdavis.edu/~gravner/MAT135A/resources/lecturenotes.pdf

% \bibitem{Prob and Comp} Mitzenmacher, Michael, and Eli Upfal. Probability and computing: Randomization and probabilistic techniques in algorithms and data analysis. Cambridge university press, 2017.
% \bibitem{Ross} Ross, Sheldon M. A first course in probability. Vol. 7. Upper Saddle River, NJ: Pearson Prentice Hall, 2006.


 
\end{thebibliography}


\end{document}